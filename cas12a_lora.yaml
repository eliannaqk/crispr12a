#save_folder: /vast/palmer/scratch/ishizuka/eqk3/crisprData/Cas12a_cdhit/checkpoints
save_folder: /vast/palmer/scratch/ishizuka/eqk3/crisprData/Cas12a_cdhit_2/checkpoints/run-20250811-121015/

data:
  sequence_col: sequence                # column in the CSV that holds the gRNA / protein sequence
  # train_data_path: /vast/palmer/scratch/ishizuka/eqk3/crisprData/Cas12a_nocdhit/train.csv
  # val_data_path:   /vast/palmer/scratch/ishizuka/eqk3/crisprData/Cas12a_nocdhit/valid.csv
  train_data_path: /vast/palmer/scratch/ishizuka/eqk3/Cas12a_cdhit/Cas12a/train.csv
  val_data_path:   /vast/palmer/scratch/ishizuka/eqk3/Cas12a_cdhit/Cas12a/valid.csv
  label_col: null                       # unsupervised LM fineâ€‘tuning
  batch_size: 8                       # sequences per GPU; this will result in a global batch size of 8
  num_workers: 6

model:
  name: progen2
  path: /vast/palmer/scratch/ishizuka/eqk3/crisprData/models/zenodo_15128064/

training:
  # Gradient accumulation simulated in YAML with GA = 8:
  # - learning_rate := learning_rate / GA
  # - warmup_steps := warmup_steps * GA
  # - train_steps := train_steps * GA
  # - save_interval_steps := save_interval_steps * GA
  learning_rate: 1e-4   # = 1e-5 / 8
  weight_decay: 0
  warmup_steps: 500      # = 500 * 8
  train_steps: 800000     # = 100000 * 8 (total optimisation steps, not epochs)
  total_lr_decay_factor: 0.2
  gradient_clipping_threshold: 1.0

# = 1000 * 8
save_interval_steps: 1000
# Evaluation cadence: once per epoch
eval_interval: 1ep
autoresume: true

algorithms:
  lora:
    r: 16
    alpha: 32
    target_modules:
      # ProGen module names: attention uses a fused qkv projection
      - qkv_proj
      - out_proj
      # MLP projections
      - fc_in
      - fc_out
    dropout: 0.05
    enabled: true